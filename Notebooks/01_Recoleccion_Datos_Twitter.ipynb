{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49891b19-6491-4706-947a-b474e75a4b40",
   "metadata": {},
   "source": [
    "# Notebook 1: Recolección de Datos\n",
    "\n",
    "## Indice\n",
    "1. Introducción y Contexto\n",
    "2. Configuracion del entorno\n",
    "3. Carga de Raw Data\n",
    "4. Extraccion de Metadata\n",
    "5. Consolidacion del Dataset\n",
    "6. Estadisticas Iniciales\n",
    "7. Guardado de Datos Procesados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6aa143-c0e8-4aa8-b66e-d6c776aa3019",
   "metadata": {},
   "source": [
    "# Recolección de Datos del Plebiscito 2020\n",
    "\n",
    "## Contexto\n",
    "Este notebook documenta el proceso de recolección y extracción de metadatos del dataset de Harvard Dataverse sobre el Plebiscito Constitucional de Chile 2020\n",
    "\n",
    "## Objetivos\n",
    "1. Cargar IDs de tweets de los 3 archivos del dataset\n",
    "2. Extraer información temporal de cada ID (Snowflake)\n",
    "3. Crear dataset consolidado con metadatos\n",
    "4. Generar estadísticas preliminares\n",
    "\n",
    "## Metodologia\n",
    "En primero momento intentamos hacer la hidratación tradicional de los datos a partir la API de X(Twitter), pero la versión gratuita solo permite hacerlo a 100 posts, y el hecho que haya que pagar para acceder a la API dificulta el acceso, por ende bajo este contexto, sumado con la probabilidad que muchos posts de esa época pueden haber sido eliminados y no sean accesibles de ver, decidimos hacer un análisis a traves de la extracción directa de timestamps desde los Snowflake IDs de X(Twitter)\n",
    "Encontramos que este enfoque, hasta esta etapa del proyecto permite tanto la disponibillidad de los datos como ayudar a responder nuestras preguntas de investigación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff5df82-50da-415a-acf7-652b28cb7222",
   "metadata": {},
   "source": [
    "# Configuracion del Entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbfb2947-3726-4dee-893e-549cc1f0fbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f099f1-42c7-48f5-8ea1-9134748fa163",
   "metadata": {},
   "source": [
    "# Carga de datos crudos\n",
    "\n",
    "**Dataset:** Plebiscito Constitucional Chile 2020  \n",
    "**Repositorio:** Harvard Dataverse  \n",
    "**DOI:** 10.7910/DVN/WGU9R9W  \n",
    "**Total IDs:** 1,704,494 tweets  \n",
    "**Período:** 14 Octubre - 26 Noviembre 2020\n",
    "\n",
    "### Archivos:\n",
    "1. Plebiscito_Constitucional_Chile_2020_01.txt (1.17M IDs)\n",
    "2. Plebiscito_Constitucional_Chile_2020_02.txt (~500k IDs)\n",
    "3. Plebiscito_Constitucional_Chile_2020_03.txt (~200k IDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72e2a8a0-2d9b-41c6-a09c-02ff6d172a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuracion de archivos\n",
    "archivos_config = [\n",
    "    {\n",
    "        'path': '../data/raw/Plebiscito_Constitucional_Chile_2020_01.txt',\n",
    "        'muestra_size': 500000,\n",
    "        'nombre': 'Archivo 1'\n",
    "    },\n",
    "    {\n",
    "        'path': '../data/raw/Plebiscito_Constitucional_Chile_2020_02.txt',\n",
    "        'muestra_size': 200000,\n",
    "        'nombre': 'Archivo 2'\n",
    "    },\n",
    "    {\n",
    "        'path': '../data/raw/Plebiscito_Constitucional_Chile_2020_03.txt',\n",
    "        'muestra_size': 100000,\n",
    "        'nombre': 'Archivo 3'\n",
    "    }\n",
    "]\n",
    "fecha_plebiscito = datetime(2020, 10, 25)\n",
    "\n",
    "# Funciones Auxiliares \n",
    "def extraer_timestamp_de_id(tweet_id):\n",
    "    \"\"\"\n",
    "    Esta función extrae el timestamp del Sonwflake ID de X (Twitter)\n",
    "    Los IDs de Twitter contienen información temporal embebida:\n",
    "    - 41 bits: timestamp (ms desde Twitter epoch) --> \"Twitter epoch\" se refiere a la fecha de inicio utilizada por el sistema de IDs de Twitter (ahora X), que es el 4 de noviembre de 2010, a las 01:42:54.657 UTC\n",
    "    - 10 bits: machine ID\n",
    "    - 12 bits: sequence number\n",
    "\n",
    "    Args:\n",
    "        tweet_id (str/int): ID del Tweet\n",
    "\n",
    "    Returns:\n",
    "        datetime: Fecha y hora del tweet/ None si hay error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        twitter_epoch = 1288834974657\n",
    "        tweet_id_int = int(tweet_id)\n",
    "        timestamp_ms = (tweet_id_int >> 22) + twitter_epoch\n",
    "        return datetime.fromtimestamp(timestamp_ms / 1000)\n",
    "    except:\n",
    "        return None\n",
    "def obtener_muestra_estratificada(tweet_ids, muestra_size):\n",
    "    \"\"\"\n",
    "    Esta función obtiene una muestra distribuida uniformemente\n",
    "\n",
    "    Implementa mustreo sistematico: toma 1 tweet cada k tweets para mantener representación\n",
    "    temporal proporcional.\n",
    "\n",
    "    Args:\n",
    "        tweet_ids (list): Lista de IDs de tweets\n",
    "        muestra_size (int): Tamaño deseado de muestra\n",
    "\n",
    "    Returns:\n",
    "        list: Muestra estratificada de IDs\n",
    "\n",
    "    Ejemplo de lo que nos referimos a estratificada:\n",
    "        >>> ids = ['1', '2', '3', '4', '5', '6']\n",
    "        >>> muestra = obtener_muestra_estratificada(ids, 3)\n",
    "        >>> print(muestra)\n",
    "        ['1', '3', '5']\n",
    "    \"\"\"\n",
    "    if len(tweet_ids) <= muestra_size:\n",
    "        return tweet_ids\n",
    "    \n",
    "    # Calcular paso para distribución uniforme\n",
    "    paso = len(tweet_ids) // muestra_size\n",
    "    \n",
    "    # Tomar tweets distribuidos uniformemente\n",
    "    muestra = []\n",
    "    for i in range(0, len(tweet_ids), paso):\n",
    "        if len(muestra) >= muestra_size:\n",
    "            break\n",
    "        muestra.append(tweet_ids[i])\n",
    "    \n",
    "    return muestra\n",
    "\n",
    "# Clasificar ventanas temporales\n",
    "def clasificar_ventana(dias):\n",
    "    if dias > 30:\n",
    "        return '4. Más de 30 días antes'\n",
    "    elif dias > 7:\n",
    "        return '3. Entre 8-30 días antes'\n",
    "    elif dias > 0:\n",
    "        return '2. Última semana (1-7 días)'\n",
    "    elif dias == 0:\n",
    "        return '1. Día del plebiscito'\n",
    "    else:\n",
    "        return '0. Post-plebiscito'\n",
    "\n",
    "# Período del día\n",
    "def clasificar_periodo_dia(hora):\n",
    "    if 5 <= hora < 12:\n",
    "        return 'Mañana (5-12h)'\n",
    "    elif 12 <= hora < 18:\n",
    "        return 'Tarde (12-18h)'\n",
    "    elif 18 <= hora < 23:\n",
    "        return 'Noche (18-23h)'\n",
    "    else:\n",
    "        return 'Madrugada (23-5h)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98340b8c-0207-4b33-90c0-c041ae326bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "PROCESANDO ARCHIVO 1 (1/3)\n",
      "======================================================================\n",
      "\n",
      " Cargando IDs de Archivo 1\n",
      "-> 1,174,133 IDs totales en el archivo\n",
      "\n",
      " Tomando muestra estratificada...\n",
      "   Objetivo: 500,000 tweets (42.6% del archivo)\n",
      " Muestra obtenida: 500,000 tweets\n",
      "   Distribución: 1 tweet cada 2 tweets\n",
      "\n",
      " Extrayendo metadatos de la muestra...\n",
      "  Progreso: 20,000/500,000 (4.0%)\n",
      "  Progreso: 40,000/500,000 (8.0%)\n",
      "  Progreso: 60,000/500,000 (12.0%)\n",
      "  Progreso: 80,000/500,000 (16.0%)\n",
      "  Progreso: 100,000/500,000 (20.0%)\n",
      "  Progreso: 120,000/500,000 (24.0%)\n",
      "  Progreso: 140,000/500,000 (28.0%)\n",
      "  Progreso: 160,000/500,000 (32.0%)\n",
      "  Progreso: 180,000/500,000 (36.0%)\n",
      "  Progreso: 200,000/500,000 (40.0%)\n",
      "  Progreso: 220,000/500,000 (44.0%)\n",
      "  Progreso: 240,000/500,000 (48.0%)\n",
      "  Progreso: 260,000/500,000 (52.0%)\n",
      "  Progreso: 280,000/500,000 (56.0%)\n",
      "  Progreso: 300,000/500,000 (60.0%)\n",
      "  Progreso: 320,000/500,000 (64.0%)\n",
      "  Progreso: 340,000/500,000 (68.0%)\n",
      "  Progreso: 360,000/500,000 (72.0%)\n",
      "  Progreso: 380,000/500,000 (76.0%)\n",
      "  Progreso: 400,000/500,000 (80.0%)\n",
      "  Progreso: 420,000/500,000 (84.0%)\n",
      "  Progreso: 440,000/500,000 (88.0%)\n",
      "  Progreso: 460,000/500,000 (92.0%)\n",
      "  Progreso: 480,000/500,000 (96.0%)\n",
      "  Progreso: 500,000/500,000 (100.0%)\n",
      "\n",
      " Procesamiento completado\n",
      "  - Tweets válidos: 500,000\n",
      "  - Errores: 0\n",
      "  - Tasa de éxito: 100.00%\n",
      "  - Período: 2020-10-07 a 2020-10-25\n",
      "  - Cobertura temporal: 18 días\n",
      "\n",
      " Total acumulado: 500,000 tweets\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO ARCHIVO 2 (2/3)\n",
      "======================================================================\n",
      "\n",
      " Cargando IDs de Archivo 2\n",
      "-> 375,436 IDs totales en el archivo\n",
      "\n",
      " Tomando muestra estratificada...\n",
      "   Objetivo: 200,000 tweets (53.3% del archivo)\n",
      " Muestra obtenida: 200,000 tweets\n",
      "   Distribución: 1 tweet cada 1 tweets\n",
      "\n",
      " Extrayendo metadatos de la muestra...\n",
      "  Progreso: 20,000/200,000 (10.0%)\n",
      "  Progreso: 40,000/200,000 (20.0%)\n",
      "  Progreso: 60,000/200,000 (30.0%)\n",
      "  Progreso: 80,000/200,000 (40.0%)\n",
      "  Progreso: 100,000/200,000 (50.0%)\n",
      "  Progreso: 120,000/200,000 (60.0%)\n",
      "  Progreso: 140,000/200,000 (70.0%)\n",
      "  Progreso: 160,000/200,000 (80.0%)\n",
      "  Progreso: 180,000/200,000 (90.0%)\n",
      "  Progreso: 200,000/200,000 (100.0%)\n",
      "\n",
      " Procesamiento completado\n",
      "  - Tweets válidos: 200,000\n",
      "  - Errores: 0\n",
      "  - Tasa de éxito: 100.00%\n",
      "  - Período: 2020-10-07 a 2020-10-25\n",
      "  - Cobertura temporal: 18 días\n",
      "\n",
      " Total acumulado: 700,000 tweets\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO ARCHIVO 3 (3/3)\n",
      "======================================================================\n",
      "\n",
      " Cargando IDs de Archivo 3\n",
      "-> 149,472 IDs totales en el archivo\n",
      "\n",
      " Tomando muestra estratificada...\n",
      "   Objetivo: 100,000 tweets (66.9% del archivo)\n",
      " Muestra obtenida: 100,000 tweets\n",
      "   Distribución: 1 tweet cada 1 tweets\n",
      "\n",
      " Extrayendo metadatos de la muestra...\n",
      "  Progreso: 20,000/100,000 (20.0%)\n",
      "  Progreso: 40,000/100,000 (40.0%)\n",
      "  Progreso: 60,000/100,000 (60.0%)\n",
      "  Progreso: 80,000/100,000 (80.0%)\n",
      "  Progreso: 100,000/100,000 (100.0%)\n",
      "\n",
      " Procesamiento completado\n",
      "  - Tweets válidos: 100,000\n",
      "  - Errores: 0\n",
      "  - Tasa de éxito: 100.00%\n",
      "  - Período: 2020-10-25 a 2020-11-03\n",
      "  - Cobertura temporal: 9 días\n",
      "\n",
      " Total acumulado: 800,000 tweets\n"
     ]
    }
   ],
   "source": [
    "# Procesar los archivos de IDs de tweets\n",
    "todos_los_datos = [] #Lista vacía donde se guardarán todos los tweets procesados de los 3 archivos\n",
    "estadisticas_archivos = [] #Lista para guardar un resumen estadístico de cada archivo\n",
    "\n",
    "for idx, config in enumerate(archivos_config, 1):\n",
    "    # voy a recorrer cada elemento de archivos_config (que contiene 3 diccionarios con la info de los 3 archivos que quiero procesar)\n",
    "    archivo = config['path']\n",
    "    muestra_size = config['muestra_size']\n",
    "    nombre = config['nombre']\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"PROCESANDO {nombre.upper()} ({idx}/3)\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    print(f\"\\n Cargando IDs de {nombre}\")\n",
    "\n",
    "    tweet_ids = [] #Lista donde se guardaran los IDs del archivo actual\n",
    "    with open(archivo, 'r', encoding='utf-8') as file:\n",
    "        for linea in file:\n",
    "            tweet_id = linea.strip()\n",
    "            if tweet_id and tweet_id.isdigit():\n",
    "                tweet_ids.append(tweet_id)\n",
    "    total_ids = len(tweet_ids)\n",
    "    print(f\"-> {total_ids:,} IDs totales en el archivo\")\n",
    "\n",
    "    # Obtener muestra estratificada\n",
    "    print(f\"\\n Tomando muestra estratificada...\")\n",
    "    print(f\"   Objetivo: {muestra_size:,} tweets ({muestra_size/total_ids*100:.1f}% del archivo)\") #Muestro que porcentaje representa la muestra del total\n",
    "    \n",
    "    muestra = obtener_muestra_estratificada(tweet_ids, muestra_size)\n",
    "    print(f\" Muestra obtenida: {len(muestra):,} tweets\")\n",
    "    print(f\"   Distribución: 1 tweet cada {total_ids//len(muestra)} tweets\")\n",
    "\n",
    "    print(f\"\\n Extrayendo metadatos de la muestra...\")\n",
    "    datos_archivo = [] #Lista para guardar los datos procesados de ESTE archivo\n",
    "    errores = 0 #contador para ids que no se pudieron procesar\n",
    "\n",
    "    for i, tweet_id in enumerate(muestra):\n",
    "        #recorro cada ID de la muestra y llamo a mi función auxiliar\n",
    "        timestamp = extraer_timestamp_de_id(tweet_id)\n",
    "        if timestamp:\n",
    "            datos_archivo.append({\n",
    "                'tweet_id': tweet_id, #el id original\n",
    "                'timestamp': timestamp, #fecha y hora completa\n",
    "                'fecha': timestamp.date(), #solo la fecha (sin hora)\n",
    "                'hora': timestamp.hour, #solo el numero de la hora 0-23\n",
    "                'dia_semana': timestamp.weekday(), #numero del dia (0=Lunes, 6=Domingo)\n",
    "                'nombre_dia': timestamp.strftime('%A'), #nombre del dia en ingles\n",
    "                'archivo_origen': nombre #de que archivo viene \n",
    "            })\n",
    "        else:\n",
    "            errores +=1\n",
    "\n",
    "    # cada 20k tweets procesados, muestro el progreso\n",
    "        if (i + 1) % 20000 == 0:\n",
    "            print(f\"  Progreso: {i+1:,}/{len(muestra):,} ({(i+1)/len(muestra)*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n Procesamiento completado\")\n",
    "    print(f\"  - Tweets válidos: {len(datos_archivo):,}\")\n",
    "    print(f\"  - Errores: {errores:,}\")\n",
    "    print(f\"  - Tasa de éxito: {len(datos_archivo)/len(muestra)*100:.2f}%\")\n",
    "\n",
    "    # Guardar estadísticas del archivo\n",
    "    if datos_archivo:\n",
    "        df_temp = pd.DataFrame(datos_archivo) # Creo un df temporal con los datos\n",
    "        # guardo las estadisticas del archivo en un diccionario\n",
    "        estadisticas_archivos.append({\n",
    "            'archivo': nombre,\n",
    "            'ids_totales': total_ids,\n",
    "            'muestra_procesada': len(datos_archivo),\n",
    "            'porcentaje_muestra': f\"{len(datos_archivo)/total_ids*100:.2f}%\",\n",
    "            'fecha_inicio': df_temp['fecha'].min(),\n",
    "            'fecha_fin': df_temp['fecha'].max(),\n",
    "            'dias_cobertura': (df_temp['fecha'].max() - df_temp['fecha'].min()).days\n",
    "        })\n",
    "\n",
    "        # Muestro el rango de fechas y cuantos dias cubren los datos recolectados\n",
    "        print(f\"  - Período: {df_temp['fecha'].min()} a {df_temp['fecha'].max()}\")\n",
    "        print(f\"  - Cobertura temporal: {(df_temp['fecha'].max() - df_temp['fecha'].min()).days} días\")\n",
    "    \n",
    "    # Agregar todos los datos de este archivo a la lista general, sin que sea un nuevo elemento en la lista\n",
    "    todos_los_datos.extend(datos_archivo)\n",
    "    # Muestro cuantos tweets se han procesado en total hasta ahora (suma de todos los archivos)\n",
    "    print(f\"\\n Total acumulado: {len(todos_los_datos):,} tweets\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c275d7c3-3c15-48ec-a0bc-3517bb195e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CONSOLIDANDO DATASET COMPLETO\n",
      "======================================================================\n",
      "\n",
      " Dataset consolidado: 800,000 tweets\n",
      "  Desde: 2020-10-07\n",
      "  Hasta: 2020-11-03\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(\"CONSOLIDANDO DATASET COMPLETO\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "if not todos_los_datos:\n",
    "    print(\"No se procesaron datos. Verifica las rutas de los archivos.\")\n",
    "    exit()\n",
    "\n",
    "df_completo = pd.DataFrame(todos_los_datos)\n",
    "\n",
    "print(f\"\\n Dataset consolidado: {len(df_completo):,} tweets\")\n",
    "print(f\"  Desde: {df_completo['fecha'].min()}\")\n",
    "print(f\"  Hasta: {df_completo['fecha'].max()}\")\n",
    "\n",
    "# Calcular días antes del plebiscito\n",
    "df_completo['dias_antes_plebiscito'] = (fecha_plebiscito - pd.to_datetime(df_completo['fecha'])).dt.days\n",
    "\n",
    "df_completo['ventana_temporal'] = df_completo['dias_antes_plebiscito'].apply(clasificar_ventana)\n",
    "\n",
    "df_completo['periodo_dia'] = df_completo['hora'].apply(clasificar_periodo_dia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48a467d2-d6e6-4bce-866e-c5ef37777baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      " ESTADÍSTICAS DEL DATASET \n",
      "======================================================================\n",
      "\n",
      "  RESUMEN GENERAL:\n",
      "   Total tweets en muestra:  800,000\n",
      "   Período completo:         2020-10-07 a 2020-11-03\n",
      "   Días de cobertura:        27\n",
      "   Variables por tweet:      10\n",
      "\n",
      "*  DESGLOSE POR ARCHIVO:\n",
      "\n",
      "*  DISTRIBUCIÓN POR VENTANA TEMPORAL:\n",
      "   0. Post-plebiscito: 70,345 tweets (8.79%)\n",
      "   1. Día del plebiscito: 151,685 tweets (18.96%)\n",
      "   2. Última semana (1-7 días): 348,989 tweets (43.62%)\n",
      "   3. Entre 8-30 días antes: 228,981 tweets (28.62%)\n",
      "\n",
      "*  TOP 10 DÍAS CON MÁS ACTIVIDAD:\n",
      "    1. 2020-10-25 (  0 días antes): 151,685 tweets\n",
      "    2. 2020-10-24 (  1 días antes): 66,310 tweets\n",
      "    3. 2020-10-18 (  7 días antes): 63,945 tweets\n",
      "    4. 2020-10-22 (  3 días antes): 46,244 tweets\n",
      "    5. 2020-10-19 (  6 días antes): 45,123 tweets\n",
      "    6. 2020-10-21 (  4 días antes): 44,800 tweets\n",
      "    7. 2020-10-20 (  5 días antes): 44,440 tweets\n",
      "    8. 2020-10-26 ( -1 días antes): 41,355 tweets\n",
      "    9. 2020-10-10 ( 15 días antes): 39,005 tweets\n",
      "   10. 2020-10-23 (  2 días antes): 38,127 tweets\n",
      "\n",
      "*  ACTIVIDAD POR HORA DEL DÍA:\n",
      "   Hora más activa:   22:00 hrs (67,963 tweets)\n",
      "   Hora menos activa: 4:00 hrs (4,282 tweets)\n",
      "\n",
      "*  DISTRIBUCIÓN POR PERÍODO DEL DÍA:\n",
      "   Madrugada (23-5h)   :  153,094 tweets (19.14%)\n",
      "   Mañana (5-12h)      :  191,105 tweets (23.89%)\n",
      "   Tarde (12-18h)      :  217,253 tweets (27.16%)\n",
      "   Noche (18-23h)      :  238,548 tweets (29.82%)\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(\" ESTADÍSTICAS DEL DATASET \")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "print(f\"\\n  RESUMEN GENERAL:\")\n",
    "print(f\"   Total tweets en muestra:  {len(df_completo):,}\")\n",
    "print(f\"   Período completo:         {df_completo['fecha'].min()} a {df_completo['fecha'].max()}\")\n",
    "print(f\"   Días de cobertura:        {(df_completo['fecha'].max() - df_completo['fecha'].min()).days}\")\n",
    "print(f\"   Variables por tweet:      {len(df_completo.columns)}\")\n",
    "\n",
    "print(f\"\\n*  DESGLOSE POR ARCHIVO:\")\n",
    "df_stats = pd.DataFrame(estadisticas_archivos)\n",
    "\n",
    "print(f\"\\n*  DISTRIBUCIÓN POR VENTANA TEMPORAL:\")\n",
    "ventanas = df_completo['ventana_temporal'].value_counts().sort_index()\n",
    "for ventana, count in ventanas.items():\n",
    "    print(f\"   {ventana}: {count:,} tweets ({count/len(df_completo)*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\n*  TOP 10 DÍAS CON MÁS ACTIVIDAD:\")\n",
    "top_dias = df_completo['fecha'].value_counts().head(10)\n",
    "\n",
    "for i, (fecha, count) in enumerate(top_dias.items(), 1):\n",
    "    dias_antes = (fecha_plebiscito.date() - fecha).days\n",
    "    print(f\"   {i:2d}. {fecha} ({dias_antes:3d} días antes): {count:6,} tweets\")\n",
    "\n",
    "print(f\"\\n*  ACTIVIDAD POR HORA DEL DÍA:\")\n",
    "hora_counts = df_completo['hora'].value_counts().sort_index()\n",
    "print(f\"   Hora más activa:   {hora_counts.idxmax()}:00 hrs ({hora_counts.max():,} tweets)\")\n",
    "print(f\"   Hora menos activa: {hora_counts.idxmin()}:00 hrs ({hora_counts.min():,} tweets)\")\n",
    "\n",
    "print(f\"\\n*  DISTRIBUCIÓN POR PERÍODO DEL DÍA:\")\n",
    "periodo_counts = df_completo['periodo_dia'].value_counts()\n",
    "for periodo in ['Madrugada (23-5h)', 'Mañana (5-12h)', 'Tarde (12-18h)', 'Noche (18-23h)']:\n",
    "    if periodo in periodo_counts.index:\n",
    "        count = periodo_counts[periodo]\n",
    "        print(f\"   {periodo:20s}: {count:8,} tweets ({count/len(df_completo)*100:.2f}%)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79779f2e-2f50-4332-ac10-36b77de86899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      " GUARDANDO DATOS\n",
      "======================================================================\n",
      "\n",
      " DataFrame guardado en CSV:\n",
      "  ../data/processed/muestra_tweets.csv\n",
      "\n",
      " DataFrame guardado en Parquet:\n",
      "  ../data/processed/muestra_tweets.parquet\n",
      "\n",
      " Estadísticas guardadas:\n",
      "  ../data/processed/estadisticas_archivos.csv\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(\" GUARDANDO DATOS\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# 1. Guardo el DataFrame principal con todos los tweets\n",
    "if todos_los_datos:\n",
    "    df_completo = pd.DataFrame(todos_los_datos)\n",
    "    \n",
    "    # Guardar en CSV\n",
    "    ruta_csv = '../data/processed/muestra_tweets.csv'\n",
    "    df_completo.to_csv(ruta_csv, index=False, encoding='utf-8')\n",
    "    print(f\"\\n DataFrame guardado en CSV:\")\n",
    "    print(f\"  {ruta_csv}\")\n",
    "\n",
    "    # Guardar en Parquet (más eficiente para cargar después)\n",
    "    ruta_parquet = '../data/processed/muestra_tweets.parquet'\n",
    "    df_completo.to_parquet(ruta_parquet, index=False)\n",
    "    print(f\"\\n DataFrame guardado en Parquet:\")\n",
    "    print(f\"  {ruta_parquet}\")\n",
    "\n",
    "# 2. Guardar estadísticas de los archivos\n",
    "if estadisticas_archivos:\n",
    "    df_stats = pd.DataFrame(estadisticas_archivos)\n",
    "    ruta_stats = '../data/processed/estadisticas_archivos.csv'\n",
    "    df_stats.to_csv(ruta_stats, index=False, encoding='utf-8')\n",
    "    print(f\"\\n Estadísticas guardadas:\")\n",
    "    print(f\"  {ruta_stats}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ea3450-2486-4691-93dd-02e48b178073",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
